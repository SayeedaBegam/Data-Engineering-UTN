{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89518eb3-26d6-49b3-a4e8-082d7c4528eb",
   "metadata": {},
   "source": [
    "# Exercise 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e9b26-4e6a-4be2-9327-98ced2ac69c7",
   "metadata": {},
   "source": [
    "Your name: ______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f9973-1430-4a69-aec1-985a3343524a",
   "metadata": {},
   "source": [
    "We have already seen in the last exercise that datasets oftemtimes contain \"dirty\" data. This can be problematic for subsequent analysis, e.g. outliers might skew results or inconsistencies might be introduced.\n",
    "\n",
    "In this assignment, we will do the following:\n",
    " - We will again work with the New York Taxi Dataset. If you still have a local copy, great! If not, download it again following the instructions from the last exercise\n",
    " - You will use [deequ](https://github.com/awslabs/deequ), the tool we discussed in a video on Canvas, to come up and implement unit tests for the NYTD dataset. Refer also to the discussion on Canvas to implement new checks!\n",
    " - As we have already seen, the whole dataset will not pass the unit tests because it contains a lot of dirty data. However, it's also not realistic that there is a once-in-a-year batch process that uploads a whole year worth of data into your pipeline. Instead, we want to implement something more realistic: we simulate smaller batches and check them with deequ one-by-one.\n",
    "\n",
    "We strongly advise you to use Google Colab for this exercise because setting up a correct environment is not trivial. There are a couple of setup instructions in the notebook. If you execute them in Google Colab, everything should just work ;-)\n",
    "\n",
    "\n",
    "We will use the Python package for deequ:\n",
    "```\n",
    "pip install pyspark\n",
    "pip install pydeequ\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfa1d6-fcbd-47a5-9798-9b97344b2ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pyspark\n",
    "!pip install pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b500cdf-304e-494c-804d-949ee9402816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java 11. You can run this code in Google Colab. But if you use your own machine, we advise you to be careful.\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "!update-alternatives --set java /usr/lib/jvm/java-11-openjdk-amd64/jre/bin/java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0e091-9a27-4aec-a54f-3b7efa56ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!java --version # This should be Java 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7237b4-adb8-4a20-9aae-f0482f66f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyspark --version # Should be Spark 3.5.x and Scala 2.12.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbb073-9aa9-406f-8c0a-d0b906f6ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download New York Taxi trip data set (again) and convert it to CSV\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet')\n",
    "df.to_csv('2021_Yellow_Taxi_Trip_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f09f9-501b-4ecc-96ae-bfcb66c78b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa52a1c-4a56-4784-89c8-db663c32d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import pydeequ\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "    .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55974e0a-3cb2-4e5f-9719-09bc1e0ec686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a dataframe with spark\n",
    "df = spark.read.option(\"header\", True).csv(\"2021_Yellow_Taxi_Trip_Data.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f42fea-8bc5-4991-b1da-7b5b7b6d6f9d",
   "metadata": {},
   "source": [
    "## Task 1: Strings to Types\n",
    "\n",
    "Oh! As you can see above, all columns are interpreted as strings. That's pretty bad to work with.\n",
    "\n",
    "Task:\n",
    "1. Take a sample of roughly 10000 rows from the dataframe containing the NYTD dataset\n",
    "2. Use deequ to run a profiling job over the the sample\n",
    "3. Extract the inferred type information from the report using Python.\n",
    "4. Automatically adjust the data types in the dataframe to reflect more fitting types\n",
    "5. Print the resulting schema\n",
    "\n",
    "Note: Outside of this exercise, you can also use Spark's `inferSchema` argument when reading a CSV, but it requires an extra pass over the data. This takes some additional time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757076a8-822b-4da5-a02a-ee5d504ea3c9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b8f4ecff987de453fe521dbbadbb359",
     "grade": true,
     "grade_id": "cell-a7248c28901582a9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Answer:\n",
    "# <your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c4830-d4df-41c6-bcb9-5a4c59d2ad3a",
   "metadata": {},
   "source": [
    "## Task 2: Batch-process Data\n",
    "\n",
    "When setting up a data pipeline, you rarely have all the data available from the beginning on. Instead, there will usually be inserts over time, oftentimes partitioned into batches. So let's simulate this.\n",
    "We will split the dataset into many small files and look at them one-by-one with deequ.\n",
    "\n",
    "Your task:\n",
    "1. Implement at least 10 simple checks with deequ's `VerificationSuite`. Make sure some files still pass the check! You might want to draw inspiration from the discussion on Canvas.\n",
    "2. Iterate over all batches and run the check on each of them. Count how many of them pass the check and output that number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dfb4b5-a8c4-4292-a520-040279680573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the given dataframe into many individual files (batches)\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "tmppath = os.path.join(tempfile.gettempdir(), 'nytd_partitions')\n",
    "\n",
    "df.write.option(\"maxRecordsPerFile\", 1000) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(tmppath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3eeca-49b5-48d8-8876-9147c018b49f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79b2f3cda28d036ae77d71c58f88f5fa",
     "grade": true,
     "grade_id": "cell-2108405e923e0cb9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Answer:\n",
    "# <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd6a9f-9b2a-4e46-96d2-2b2f4f095758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Spark session\n",
    "spark.sparkContext._gateway.shutdown_callback_server()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a1a52-c23e-4932-851f-3fce0e2d7a13",
   "metadata": {},
   "source": [
    "## Feedback (voluntary)\n",
    "\n",
    "How did you like this exercise? What could be improved?\n",
    "\n",
    "Answer:\n",
    "\n",
    "...\n",
    "\n",
    "Further, I feel like:\n",
    " - [ ] the exercise was too easy\n",
    " - [ ] the exercise was too hard\n",
    " - [ ] the exercise was just right\n",
    " - [x] no answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
